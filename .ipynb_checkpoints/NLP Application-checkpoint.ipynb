{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "large-aberdeen",
   "metadata": {},
   "source": [
    "## 1. Import libraries and requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "opponent-meditation",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Datasets\n"
     ]
    }
   ],
   "source": [
    "# Loading required packages\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import csv\n",
    "import ast\n",
    "\n",
    "# Select Spacy model\n",
    "# Efficiency\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Accuracy\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Change working directory\n",
    "%cd '/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "earned-production",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-activity",
   "metadata": {},
   "source": [
    "## 2. Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-flower",
   "metadata": {},
   "source": [
    "### Load EMSCAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "toxic-prefix",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17880"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the csv file into the environment\n",
    "jobdescriptions = pd.read_csv('EMSCAD/Input data/JobDescriptions.csv', delimiter=',')\n",
    "jobdescriptions.head(5)\n",
    "len(jobdescriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-inspection",
   "metadata": {},
   "source": [
    "### Subset data, remain only the column description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "general-display",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;p&gt;Food52, a fast-growing, James Beard Award-w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;p&gt;Organised - Focused - Vibrant - Awesome!&lt;br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;p&gt;Our client, located in Houston, is actively...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;p&gt;&lt;b&gt;THE COMPANY: ESRI – Environmental System...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;p&gt;&lt;b&gt;JOB TITLE:&lt;/b&gt; Itemization Review Manage...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description\n",
       "0  <p>Food52, a fast-growing, James Beard Award-w...\n",
       "1  <p>Organised - Focused - Vibrant - Awesome!<br...\n",
       "2  <p>Our client, located in Houston, is actively...\n",
       "3  <p><b>THE COMPANY: ESRI – Environmental System...\n",
       "4  <p><b>JOB TITLE:</b> Itemization Review Manage..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy column description from DF jobdescription\n",
    "descriptions = jobdescriptions['description']\n",
    "\n",
    "# Convert Series into Dataframe\n",
    "descriptions = descriptions.to_frame()\n",
    "descriptions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-administration",
   "metadata": {},
   "source": [
    "## 3. Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-journey",
   "metadata": {},
   "source": [
    "### Remove HTML patterns in job descriptions\n",
    "\n",
    "### Once cleaned, we can put the data through Spacy's NLP pipeline and tokenize each description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dental-clause",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15136\n",
      "CPU times: user 406 ms, sys: 19.5 ms, total: 426 ms\n",
      "Wall time: 426 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Food52, a fast-growing, James Beard Award-winn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Organised - Focused - Vibrant - Awesome!Do you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our client, located in Houston, is actively se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE COMPANY: ESRI – Environmental Systems Rese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JOB TITLE: Itemization Review Manager \\nLOCATI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Result\n",
       "0  Food52, a fast-growing, James Beard Award-winn...\n",
       "1  Organised - Focused - Vibrant - Awesome!Do you...\n",
       "2  Our client, located in Houston, is actively se...\n",
       "3  THE COMPANY: ESRI – Environmental Systems Rese...\n",
       "4  JOB TITLE: Itemization Review Manager \\nLOCATI..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Remove HTML codes based on pattern cleanr\n",
    "# Remove all characters except whitespace an alphabetic characters.\n",
    "\n",
    "result = []\n",
    "docDF = pd.DataFrame()\n",
    "max_length = 0\n",
    "for i in descriptions[\"description\"]:\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    i = re.sub(cleanr, '', i)\n",
    "    cleanr = re.compile('<[^>]+>')\n",
    "    i = re.sub(cleanr, '', i)\n",
    "    i = i.replace('\\xa0', ' ')\n",
    "    i = i.replace('\\r', ' ')\n",
    "    i = i.replace('&amp', ' ')\n",
    "    i = i.replace('\\N{SOFT HYPHEN}', '')\n",
    "    doc = i\n",
    "    if len(doc) > max_length:\n",
    "        max_length = len(doc)\n",
    "    result.append(doc)\n",
    "\n",
    "print(max_length)\n",
    "    \n",
    "# Add the result\n",
    "docDF[\"Result\"] = result\n",
    "docDF.head(5)\n",
    "\n",
    "# %%time\n",
    "# # Remove HTML codes based on pattern cleanr\n",
    "# # Remove all characters except whitespace an alphabetic characters.\n",
    "\n",
    "# result = []\n",
    "# docDF = pd.DataFrame()\n",
    "# count = 0\n",
    "# for i in descriptions[\"description\"]:\n",
    "#     cleanr = re.compile('<.*?>')\n",
    "#     i = re.sub(cleanr, '', i)\n",
    "#     cleanr = re.compile('<[^>]+>')\n",
    "#     i = re.sub(cleanr, '', i)\n",
    "#     i = i.replace('\\xa0', ' ')\n",
    "#     i = i.replace('\\r', ' ')\n",
    "#     i = i.replace('&amp', ' ')\n",
    "#     i = i.replace('\\N{SOFT HYPHEN}', '')\n",
    "#     if len(i)>512:\n",
    "#         text = i[:512]\n",
    "#         count = count +1\n",
    "#     else:\n",
    "#         text = i\n",
    "#     doc = nlp(str(text))\n",
    "#     result.append(doc)\n",
    "# print(count)\n",
    "\n",
    "# # Add the result\n",
    "# docDF[\"Result\"] = result\n",
    "# docDF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-india",
   "metadata": {},
   "source": [
    "### Split descriptions into sentences\n",
    "### By doing so, we prepare the data for annotation and training the custom NER model ---> Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accurate-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "# # for sent in doc.sents:\n",
    "# #     print(sent.text)\n",
    "\n",
    "# for i in docDF[\"Result\"]:\n",
    "#     for sent in i.sents:\n",
    "# #         print(sent.text)\n",
    "#         result.append(sent.text)\n",
    "    \n",
    "# sentences = pd.DataFrame(columns=['sentence'])\n",
    "# sentences[\"sentence\"] = result\n",
    "\n",
    "# type(sentences[\"sentence\"])\n",
    "\n",
    "# sentences[\"sentence\"].head(10)\n",
    "\n",
    "# sentences['sentence'].to_csv('EMSCAD/Output data/sentence.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "continuous-criterion",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.48 s, sys: 108 ms, total: 3.59 s\n",
      "Wall time: 3.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Split each description into sentences\n",
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in docDF[\"Result\"]:\n",
    "    i = str(i)\n",
    "    sentences = split_into_sentences(i)\n",
    "    if len(sentences)>0:\n",
    "        result.append(sentences)\n",
    "\n",
    "sentences = pd.DataFrame(columns=['sentence'])\n",
    "sentences[\"sentence\"] = result\n",
    "sentences['sentence'].to_csv('EMSCAD/Output data/sentence.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-offense",
   "metadata": {},
   "source": [
    "# SKIP CLEANING PROCESS DOWN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exceptional-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to skip the cleaning process\n",
    "# sentences = pd.read_csv('EMSCAD/Output data/sentence.csv', delimiter=',', names = ['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "organized-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% / 20% split\n",
    "Train, Eval = train_test_split(sentences, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-feature",
   "metadata": {},
   "source": [
    "### Now that we created sentences, we can also see that not everything went right\n",
    "### We remove each \"sentence\" that begins with a prefix: \".\"\n",
    "### This action takes care of removing all invalid sentences from the dataset, for each description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tight-biology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.4 ms, sys: 3.4 ms, total: 57.8 ms\n",
      "Wall time: 57.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Train_list = []\n",
    "Eval_list = []\n",
    "def remove_invalid_sentences(sentences):\n",
    "    prefixes = ('.')\n",
    "    tokens = []\n",
    "    output = []\n",
    "    for i in sentences[\"sentence\"]:\n",
    "        tokens = [token for token in i if not token.startswith(prefixes)]\n",
    "        output.append(tokens)\n",
    "    return output\n",
    "\n",
    "Train_list = remove_invalid_sentences(Train)\n",
    "\n",
    "Eval_list = remove_invalid_sentences(Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "competent-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(docDF)\n",
    "\n",
    "# Cleaned = pd.DataFrame()\n",
    "# type(Cleaned)\n",
    "\n",
    "# Pos = pd.DataFrame()\n",
    "# type(Pos)\n",
    "# # cols = ['DocObject']\n",
    "# # tempdocDF = pd.DataFrame(columns=cols)\n",
    "# # type(tempdocDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "surface-disorder",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Niet meer nodig omdat speciale tekens behouden blijven vanwege de zinnen.\n",
    "\n",
    "# # %%time\n",
    "\n",
    "# # Use Spacy to select the tokens using criteria\n",
    "# result = []\n",
    "# for i in docDF[\"Result\"]:\n",
    "#     i = [token.orth_ for token in i if not token.is_punct | token.is_space | token.is_stop | token.is_bracket | token.like_url | token.like_email | token.is_digit | token.is_currency] \n",
    "#     result.append(i)\n",
    "# Cleaned[\"Result\"] = result   \n",
    "# print(Cleaned)\n",
    "\n",
    "\n",
    "# # %%time\n",
    "\n",
    "# # # Use Spacy to select the tokens using criteria\n",
    "# # result = []\n",
    "# # for i in docDF[\"Result\"]:\n",
    "# #     i = [token.orth_ for token in i if not token.is_punct | token.is_space | token.is_stop | token.is_bracket | token.like_url | token.like_email | token.is_digit | token.is_currency] \n",
    "# #     result.append(i)\n",
    "# # Cleaned[\"Result\"] = result   \n",
    "# # print(Cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-match",
   "metadata": {},
   "source": [
    "## 4. Prepare TRAIN_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-princess",
   "metadata": {},
   "source": [
    "### We do this by splitting each sentence on tokens and putting them all in one column, creating a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "whole-baseball",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (931 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10037\n",
      "15128\n",
      "2614\n",
      "5806\n",
      "CPU times: user 3h 55min 33s, sys: 4min 20s, total: 3h 59min 53s\n",
      "Wall time: 1h 9min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Train_Annotation_data = pd.DataFrame(pd.DataFrame(columns=['Result', 'Label']))\n",
    "Eval_Annotation_data = pd.DataFrame(pd.DataFrame(columns=['Result', 'Label']))\n",
    "#########################\n",
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "#########################\n",
    "\n",
    "prefixes = [\"\\\"\",\"#\",\"$\",\"%\",\"&\",\"'\",\"(\",\")\",\"*\",\"+\",\",\",\" \",\"-\",\"/\",\":\",\";\",\"<\",\"=\",\">\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\"`\",\"{\",\"|\",\"}\",\"~\"]\n",
    "prefixes_end = [\"?\",\"!\"]\n",
    "full_stop = [\".\"]\n",
    "\n",
    "def sentence_to_words(input_list):\n",
    "    prefixes = [\"\\\"\",\"#\",\"$\",\"%\",\"&\",\"'\",\"(\",\")\",\"*\",\"+\",\",\",\" \",\"-\",\"/\",\":\",\";\",\"<\",\"=\",\">\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\"`\",\"{\",\"|\",\"}\",\"~\"]\n",
    "    prefixes_end = [\"?\",\"!\"]\n",
    "    full_stop = [\".\"]\n",
    "    result = []\n",
    "    max_length = 0\n",
    "    count = 0\n",
    "    for i in input_list:\n",
    "        i = str(i)\n",
    "        ######################### USE FOR en_core_web_trf model only!\n",
    "        if len(i) > max_length:\n",
    "            max_length = len(i)\n",
    "        if len(i)>512:\n",
    "            i = i[:512]\n",
    "            count = count +1\n",
    "        #########################\n",
    "        i = nlp(i)\n",
    "        for token in i:\n",
    "            #########################\n",
    "            token = token.lemma_\n",
    "            #########################\n",
    "            if str(token) not in prefixes and str(token) not in prefixes_end:\n",
    "                result.append(token)\n",
    "            elif str(token) in prefixes_end:\n",
    "                result.append(nlp(full_stop[0]))\n",
    "    print(count)\n",
    "    print(max_length)\n",
    "    return result\n",
    "\n",
    "Train_Annotation_data[\"Result\"] = sentence_to_words(Train_list)\n",
    "Eval_Annotation_data[\"Result\"] = sentence_to_words(Eval_list)\n",
    "\n",
    "# %%time\n",
    "# result = []\n",
    "# Annotation_data = pd.DataFrame(pd.DataFrame(columns=['Result', 'Label']))\n",
    "# #########################\n",
    "# # lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "\n",
    "\n",
    "# prefixes = [\"\\\"\",\"#\",\"$\",\"%\",\"&\",\"'\",\"(\",\")\",\"*\",\"+\",\",\",\" \",\"-\",\"/\",\":\",\";\",\"<\",\"=\",\">\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\"`\",\"{\",\"|\",\"}\",\"~\"]\n",
    "# prefixes_end = [\"?\",\"!\"]\n",
    "# full_stop = [\".\"]\n",
    "# # test = raw_input(test)\n",
    "# # full_stop[0] = full_stop[0].replace('\"', '')\n",
    "# for i in tokens_total:\n",
    "#     i = str(i)\n",
    "#     i = nlp(i)\n",
    "# #     print(i)\n",
    "#     for token in i:\n",
    "#         #########################\n",
    "# #         token = token.lemma_\n",
    "#         if str(token) not in prefixes and str(token) not in prefixes_end:\n",
    "#             result.append(token)\n",
    "#         elif str(token) in prefixes_end:\n",
    "#             result.append(nlp(full_stop[0]))\n",
    "# #         print(token)\n",
    "            \n",
    "# Annotation_data[\"Result\"] = result\n",
    "# Annotation_data[\"Result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "incident-decade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to be annotated (Can be used for manual annotation!)\n",
    "Train_Annotation_data['Result'].to_csv('EMSCAD/Output data/Train_Annotation_data.csv')\n",
    "Eval_Annotation_data['Result'].to_csv('EMSCAD/Output data/Eval_Annotation_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "recognized-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the source file that contains all biased word lists\n",
    "biased_words = pd.read_csv('EMSCAD/Input data/biased_words.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "champion-macedonia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8629 words have been annotated.\n",
      "2223 words have been annotated.\n",
      "CPU times: user 1min 52s, sys: 1.19 s, total: 1min 53s\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Automated annotation process (Based on the word lists imported.)\n",
    "# Only exact matches will be annotated.\n",
    "def automated_annotation(Annotation_data):\n",
    "    result = []\n",
    "    row = -1\n",
    "    count = 0\n",
    "    for i in Annotation_data['Result']:\n",
    "        i = str(i)\n",
    "        row = row + 1\n",
    "        for j in biased_words:\n",
    "            for k in biased_words[j]:\n",
    "                word = str(k)\n",
    "                if word == i:\n",
    "                    Annotation_data['Label'][row] = j\n",
    "                    count = count + 1\n",
    "    Annotation_data['Label'] = Annotation_data['Label'].fillna(\"O\")\n",
    "    print(str(count) + \" words have been annotated.\")\n",
    "    return Annotation_data\n",
    "\n",
    "Train_Annotation_data = automated_annotation(Train_Annotation_data)\n",
    "Eval_Annotation_data = automated_annotation(Eval_Annotation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "changing-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export annotated data (By \"automated\" annotator)\n",
    "Train_Annotation_data.to_csv('EMSCAD/Output data/Train_Annotation_data_output.tsv', sep='\\t', index = False, header = False)\n",
    "Eval_Annotation_data.to_csv('EMSCAD/Output data/Eval_Annotation_data_output.tsv', sep='\\t', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-gabriel",
   "metadata": {},
   "source": [
    "### Convert tsv to JSON format --> Used to be the input for Spacy V2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "historic-description",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.22 s, sys: 97.3 ms, total: 3.32 s\n",
      "Wall time: 3.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "def tsv_to_json_format(input_path,output_path,unknown_label):\n",
    "    try:\n",
    "        f=open(input_path,'r') # input file\n",
    "        fp=open(output_path, 'w') # output file\n",
    "        data_dict={}\n",
    "        annotations =[]\n",
    "        label_dict={}\n",
    "        s=''\n",
    "        start=0\n",
    "        for line in f:\n",
    "            if line[0:len(line)-1]!='.\\tO':\n",
    "                word,entity=line.split('\\t')\n",
    "                s+=word+\" \"\n",
    "                entity=entity[:len(entity)-1]\n",
    "                if entity!=unknown_label:\n",
    "                    if len(entity) != 1:\n",
    "                        d={}\n",
    "                        d['text']=word\n",
    "                        d['start']=start\n",
    "                        d['end']=start+len(word)-1\n",
    "                        try:\n",
    "                            label_dict[entity].append(d)\n",
    "                        except:\n",
    "                            label_dict[entity]=[]\n",
    "                            label_dict[entity].append(d)\n",
    "                start+=len(word)+1\n",
    "            else:\n",
    "                data_dict['content']=s\n",
    "                s=''\n",
    "                label_list=[]\n",
    "                for ents in list(label_dict.keys()):\n",
    "                    for i in range(len(label_dict[ents])):\n",
    "                        if(label_dict[ents][i]['text']!=''):\n",
    "                            l=[ents,label_dict[ents][i]]\n",
    "                            for j in range(i+1,len(label_dict[ents])): \n",
    "                                if(label_dict[ents][i]['text']==label_dict[ents][j]['text']):  \n",
    "                                    di={}\n",
    "                                    di['start']=label_dict[ents][j]['start']\n",
    "                                    di['end']=label_dict[ents][j]['end']\n",
    "                                    di['text']=label_dict[ents][i]['text']\n",
    "                                    l.append(di)\n",
    "                                    label_dict[ents][j]['text']=''\n",
    "                            label_list.append(l)                         \n",
    "                            \n",
    "                for entities in label_list:\n",
    "                    label={}\n",
    "                    label['label']=[entities[0]]\n",
    "                    label['points']=entities[1:]\n",
    "                    annotations.append(label)\n",
    "                data_dict['annotation']=annotations\n",
    "                annotations=[]\n",
    "                json.dump(data_dict, fp)\n",
    "                fp.write('\\n')\n",
    "                data_dict={}\n",
    "                start=0\n",
    "                label_dict={}\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process file\" + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "\n",
    "tsv_to_json_format('EMSCAD/Output data/Train_Annotation_data_output.tsv','EMSCAD/Input model/Train_Annotation_data_model_input.json','abc')\n",
    "tsv_to_json_format('EMSCAD/Output data/Eval_Annotation_data_output.tsv','EMSCAD/Input model/Eval_Annotation_data_model_input.json','abc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-channels",
   "metadata": {},
   "source": [
    "# Convert JSON to SpaCy V2 format\n",
    "## We need to do this by OS Terminal!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-defeat",
   "metadata": {},
   "source": [
    "# Convert SpaCy V2 format --> SpaCy V3 format\n",
    "### First we need to import the data the data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cubic-amsterdam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import file\n",
    "Train_Spacy_file = open(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Datasets/EMSCAD/Input model/Train_Spacy_v2_format.txt\", \"r\")\n",
    "Eval_Spacy_file = open(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Datasets/EMSCAD/Input model/Eval_Spacy_v2_format.txt\", \"r\")\n",
    "TRAIN_DATA = ast.literal_eval(Train_Spacy_file.read())\n",
    "EVAL_DATA = ast.literal_eval(Eval_Spacy_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We transform the imported file back to tuple format.\n",
    "\n",
    "# TRAIN_DATA = ast.literal_eval(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-collection",
   "metadata": {},
   "source": [
    "### Actual conversion of the data to SpaCy V3 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "suffering-connectivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42947/42947 [00:43<00:00, 983.93it/s] \n",
      "100%|██████████| 11573/11573 [00:12<00:00, 963.52it/s] \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "def SpaCy_v3_format(DATA,FILENAME):\n",
    "    nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "    db = DocBin() # create a DocBin object\n",
    "\n",
    "    for text, annot in tqdm(DATA): # data in previous format\n",
    "        doc = nlp.make_doc(text) # create doc object from text\n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print(\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents # label the text with the ents\n",
    "        db.add(doc)\n",
    "\n",
    "    db.to_disk(FILENAME) # save the docbin object\n",
    "    \n",
    "\n",
    "SpaCy_v3_format(TRAIN_DATA,\"EMSCAD/TRAIN_EVAL_DATA/train.spacy\")\n",
    "SpaCy_v3_format(EVAL_DATA,\"EMSCAD/TRAIN_EVAL_DATA/eval.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-accountability",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "emerging-austria",
   "metadata": {},
   "source": [
    "# Model evaluation is done down here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-south",
   "metadata": {},
   "source": [
    "### There is a evaluation cell for each model. We make use of evaluation data that is not seen by the model during training. The job description used for evaluation contains biased words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation data\n",
    "Eval_file = open(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Model evaluation/Evaluation dataset.txt\", \"r\")\n",
    "EVAL_DATA = Eval_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-washer",
   "metadata": {},
   "source": [
    "### Evaluation - Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_1 = spacy.load(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Model training/Model 1/output/model-best/\") #load the best model\n",
    "doc = Model_1(EVAL_DATA) # input sample text\n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-championship",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "handed-abortion",
   "metadata": {},
   "source": [
    "### Evaluation - Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_2 = spacy.load(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Model training/Model 2/output/model-best/\") #load the best model\n",
    "doc = Model_2(EVAL_DATA) # input sample text\n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-laugh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sporting-removal",
   "metadata": {},
   "source": [
    "### Evaluation - Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_3 = spacy.load(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Model training/Model 3/output/model-best/\") #load the best model\n",
    "doc = Model_3(EVAL_DATA) # input sample text\n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-edition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "universal-thomson",
   "metadata": {},
   "source": [
    "### Evaluation - Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_4 = spacy.load(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Model training/Model 4/output/model-best/\") #load the best model\n",
    "doc = Model_4(EVAL_DATA) # input sample text\n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-closing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "compact-graduate",
   "metadata": {},
   "source": [
    "### Evaluation - Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-regard",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_5 = spacy.load(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Model training/Model 5/output/model-best/\") #load the best model\n",
    "doc = Model_5(EVAL_DATA) # input sample text\n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-staff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "occupational-there",
   "metadata": {},
   "source": [
    "### Evaluation - Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_6 = spacy.load(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Model training/Model 6/output/model-best/\") #load the best model\n",
    "doc = Model_6(EVAL_DATA) # input sample text\n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-district",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-investigation",
   "metadata": {},
   "source": [
    "### Evaluation - Model 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_7 = spacy.load(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Model training/Model 7/output/model-best/\") #load the best model\n",
    "doc = Model_7(EVAL_DATA) # input sample text\n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-diesel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "italic-behalf",
   "metadata": {},
   "source": [
    "### Evaluation - Model 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-assault",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_8 = spacy.load(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Model training/Model 8/output/model-best/\") #load the best model\n",
    "doc = Model_8(EVAL_DATA) # input sample text\n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-antigua",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-speaker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-shoulder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-retreat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-festival",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-heavy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-perfume",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-fields",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-colorado",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-accused",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-facial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-finland",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TRAIN_DATA[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(\"ner\", before=\"lemmatizer\")\n",
    "\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spacy(data, iterations):\n",
    "    TRAIN_DATA = data\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        print(ner)\n",
    "        nlp.add_pipe(\"ner\", last=True)\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(iterations):\n",
    "            print(\"Starting iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                nlp.update(\n",
    "                [text],\n",
    "                [annotations],\n",
    "                drop=0.2,\n",
    "                sgd=optimizer,\n",
    "                losses=losses\n",
    "                )\n",
    "                print(losses)\n",
    "    return(nlp)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = TRAIN_DATA_1\n",
    "\n",
    "nlp = train_spacy(TRAIN_DATA, 30)\n",
    "nlp.to_disk(\"ner_model_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-testing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-oxygen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-length",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-volleyball",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-sucking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-growing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-painting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-great",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-lunch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-tennessee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"en_core_web_sm\"\n",
    "\n",
    "# Setting up the pipeline and entity recognizer.\n",
    "if model is not None:\n",
    "    nlp = spacy.load(model)  # load existing spacy model\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    print(\"Created blank 'en' model\")\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner)\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = 'Masculine-coded words'\n",
    "LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add new entity labels to entity recognizer\n",
    "for i in LABEL:\n",
    "    ner.add_label(i)\n",
    "# Inititalizing optimizer\n",
    "if model is None:\n",
    "    optimizer = nlp.begin_training()\n",
    "else:\n",
    "    optimizer = nlp.entity.create_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import pickle\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "# New entity labels\n",
    "# Specify the new entity labels which you want to add here\n",
    "LABEL = ['I-geo', 'B-geo', 'I-art', 'B-art', 'B-tim', 'B-nat', 'B-eve', 'O', 'I-per', 'I-tim', 'I-nat', 'I-eve', 'B-per', 'I-org', 'B-gpe', 'B-org', 'I-gpe']\n",
    "\n",
    "\"\"\"\n",
    "geo = Geographical Entity\n",
    "org = Organization\n",
    "per = Person\n",
    "gpe = Geopolitical Entity\n",
    "tim = Time indicator\n",
    "art = Artifact\n",
    "eve = Event\n",
    "nat = Natural Phenomenon\n",
    "\"\"\"\n",
    "# Loading training data \n",
    "with open ('Data/ner_corpus_260', 'rb') as fp:\n",
    "    TRAIN_DATA = pickle.load(fp)\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    new_model_name=(\"New model name for model meta.\", \"option\", \"nm\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int))\n",
    "\n",
    "def main(model=None, new_model_name='new_model', output_dir=None, n_iter=10):\n",
    "    \"\"\"Setting up the pipeline and entity recognizer, and training the new entity.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spacy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    for i in LABEL:\n",
    "        ner.add_label(i)   # Add new entity labels to entity recognizer\n",
    "\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # Get names of other pipes to disable them during training to train only NER\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "            print('Losses', losses)\n",
    "\n",
    "    # Test the trained model\n",
    "    test_text = 'Gianni Infantino is the president of FIFA.'\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # Save model \n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # Test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        doc2 = nlp2(test_text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Jcharis/Natural-Language-Processing-Tutorials/blob/master/Training%20the%20Named%20Entity%20Recognizer%20in%20SpaCy.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-edward",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new entity label\n",
    "LABEL = 'Masculine-coded words'\n",
    "LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = Spacy_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    new_model_name=(\"New model name for model meta.\", \"option\", \"nm\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int))\n",
    "\n",
    "\n",
    "def main(model=None, new_model_name='Masculine-coded words', output_dir=None, n_iter=20):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    ner.add_label(LABEL)   # add new entity label to entity recognizer\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        # Note that 'begin_training' initializes the models, so it'll zero out\n",
    "        # existing entity types.\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in tqdm(TRAIN_DATA):\n",
    "                nlp.update([text], [annotations], sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "            print(losses)\n",
    "\n",
    "    # test the trained model\n",
    "    test_text = 'Do you like horses?'\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        doc2 = nlp2(test_text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     plac.call(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-alabama",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run our Function\n",
    "main(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-sustainability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training additional entity types using spaCy\n",
    "from __future__ import unicode_literals, print_function\n",
    "import pickle\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "# New entity labels\n",
    "# Specify the new entity labels which you want to add here\n",
    "LABEL = ['Masculine-coded words', 'Feminine-coded words']\n",
    "\n",
    "\"\"\"\n",
    "geo = Geographical Entity\n",
    "org = Organization\n",
    "per = Person\n",
    "gpe = Geopolitical Entity\n",
    "tim = Time indicator\n",
    "art = Artifact\n",
    "eve = Event\n",
    "nat = Natural Phenomenon\n",
    "\"\"\"\n",
    "# Loading training data \n",
    "with open ('/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Datasets/EMSCAD/Input model/Annotation_data_model_input_spacy.txt', 'rb') as fp:\n",
    "    TRAIN_DATA = pickle.load(fp)\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    new_model_name=(\"New model name for model meta.\", \"option\", \"nm\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int))\n",
    "\n",
    "def main(model=None, new_model_name='new_model', output_dir=None, n_iter=10):\n",
    "    \"\"\"Setting up the pipeline and entity recognizer, and training the new entity.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spacy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    for i in LABEL:\n",
    "        ner.add_label(i)   # Add new entity labels to entity recognizer\n",
    "\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # Get names of other pipes to disable them during training to train only NER\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "            print('Losses', losses)\n",
    "\n",
    "    # Test the trained model\n",
    "    test_text = 'Gianni Infantino is the president of FIFA.'\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # Save model \n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # Test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        doc2 = nlp2(test_text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    plac.call(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
