{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "distant-integral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Datasets\n"
     ]
    }
   ],
   "source": [
    "# Loading required packages\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Select Spacy model\n",
    "# Efficiency\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Accuracy\n",
    "# nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Change working directory\n",
    "%cd '/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-princess",
   "metadata": {},
   "source": [
    "## 2. Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-elimination",
   "metadata": {},
   "source": [
    "### Load EMSCAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becoming-milton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17880"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the csv file into the environment\n",
    "jobdescriptions = pd.read_csv('EMSCAD/Input data/JobDescriptions.csv', delimiter=',')\n",
    "jobdescriptions.head(5)\n",
    "len(jobdescriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-agency",
   "metadata": {},
   "source": [
    "### Subset data, remain only the column description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "civil-vermont",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;p&gt;Food52, a fast-growing, James Beard Award-w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;p&gt;Organised - Focused - Vibrant - Awesome!&lt;br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;p&gt;Our client, located in Houston, is actively...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;p&gt;&lt;b&gt;THE COMPANY: ESRI – Environmental System...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;p&gt;&lt;b&gt;JOB TITLE:&lt;/b&gt; Itemization Review Manage...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description\n",
       "0  <p>Food52, a fast-growing, James Beard Award-w...\n",
       "1  <p>Organised - Focused - Vibrant - Awesome!<br...\n",
       "2  <p>Our client, located in Houston, is actively...\n",
       "3  <p><b>THE COMPANY: ESRI – Environmental System...\n",
       "4  <p><b>JOB TITLE:</b> Itemization Review Manage..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy column description from DF jobdescription\n",
    "descriptions = jobdescriptions['description']\n",
    "\n",
    "# Convert Series into Dataframe\n",
    "descriptions = descriptions.to_frame()\n",
    "descriptions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-velvet",
   "metadata": {},
   "source": [
    "## 3. Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-fruit",
   "metadata": {},
   "source": [
    "### Remove HTML patterns in job descriptions\n",
    "\n",
    "### Once cleaned, we can put the data through Spacy's NLP pipeline and tokenize each description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "weird-oregon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 376 ms, sys: 30.3 ms, total: 406 ms\n",
      "Wall time: 411 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Food52, a fast-growing, James Beard Award-winn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Organised - Focused - Vibrant - Awesome!Do you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our client, located in Houston, is actively se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE COMPANY: ESRI – Environmental Systems Rese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JOB TITLE: Itemization Review Manager \\nLOCATI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Result\n",
       "0  Food52, a fast-growing, James Beard Award-winn...\n",
       "1  Organised - Focused - Vibrant - Awesome!Do you...\n",
       "2  Our client, located in Houston, is actively se...\n",
       "3  THE COMPANY: ESRI – Environmental Systems Rese...\n",
       "4  JOB TITLE: Itemization Review Manager \\nLOCATI..."
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Remove HTML codes based on pattern cleanr\n",
    "# Remove all characters except whitespace an alphabetic characters.\n",
    "\n",
    "result = []\n",
    "Cleaned = pd.DataFrame()\n",
    "for i in descriptions[\"description\"]:\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    i = re.sub(cleanr, '', i)\n",
    "    cleanr = re.compile('<[^>]+>')\n",
    "    i = re.sub(cleanr, '', i)\n",
    "    i = i.replace('\\xa0', ' ')\n",
    "    i = i.replace('\\r', ' ')\n",
    "    i = i.replace('&amp', ' ')\n",
    "    i = i.replace('\\N{SOFT HYPHEN}', '')\n",
    "    result.append(i)\n",
    "\n",
    "# Add the result\n",
    "Cleaned[\"Result\"] = result\n",
    "Cleaned.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-tournament",
   "metadata": {},
   "source": [
    "### Split descriptions into sentences\n",
    "### By doing so, we prepare the data for annotation and training the custom NER model ---> Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "urban-conversion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.21 s, sys: 72.5 ms, total: 3.29 s\n",
      "Wall time: 3.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Split each description into sentences\n",
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "endpoint = ('.')\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [token for token in sentences if not token.startswith(prefixes)]\n",
    "    return sentences\n",
    "\n",
    "def remove_invalid_sentences(sentences):\n",
    "    prefixes = ('.')\n",
    "    output = []\n",
    "    sentences = [token for token in sentences if not token.startswith(prefixes)]\n",
    "    output.append(sentences)\n",
    "    return output\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in Cleaned[\"Result\"]:\n",
    "    sentences = str(i)\n",
    "    sentences = split_into_sentences(sentences)\n",
    "#     sentences = remove_invalid_sentences(sentences)\n",
    "    if sentences:\n",
    "        result.append(sentences)\n",
    "\n",
    "sentences = pd.DataFrame(columns=['sentence'])\n",
    "sentences[\"sentence\"] = result\n",
    "sentences['sentence'].to_csv('EMSCAD/Output data/sentence.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-inspector",
   "metadata": {},
   "source": [
    "### TRAIN/ TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "moved-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% / 20% split\n",
    "Train, Eval = train_test_split(sentences, test_size=0.2, shuffle=False)\n",
    "Train = list(Train['sentence'])\n",
    "Eval = list(Eval['sentence'])\n",
    "FULL = list(sentences['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-wichita",
   "metadata": {},
   "source": [
    "## 4. Preparation and annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "social-bermuda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min, sys: 1min 49s, total: 17min 50s\n",
      "Wall time: 17min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Train_Annotation_data = pd.DataFrame(columns=['Result', 'Label'])\n",
    "Eval_Annotation_data = pd.DataFrame(columns=['Result', 'Label'])\n",
    "FULL_Annotation_data = pd.DataFrame(columns=['Result', 'Label'])\n",
    "#########################\n",
    "# lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "#########################\n",
    "\n",
    "prefixes = [\"\\\"\",\"#\",\"$\",\"%\",\"&\",\"'\",\"(\",\")\",\"*\",\"+\",\",\",\" \",\"-\",\"/\",\":\",\";\",\"<\",\"=\",\">\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\"`\",\"{\",\"|\",\"}\",\"~\"]\n",
    "prefixes_end = [\"?\",\"!\"]\n",
    "full_stop = [\".\"]\n",
    "\n",
    "def sentence_to_words(input_list):\n",
    "    prefixes = [\"\\\"\",\"#\",\"$\",\"%\",\"&\",\"'\",\"(\",\")\",\"*\",\"+\",\",\",\" \",\"-\",\"/\",\":\",\";\",\"<\",\"=\",\">\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\"`\",\"{\",\"|\",\"}\",\"~\"]\n",
    "    prefixes_end = [\"?\",\"!\"]\n",
    "    full_stop = [\".\"]\n",
    "    result = []\n",
    "    max_length = 0\n",
    "    count = 0\n",
    "    for i in input_list:\n",
    "        i = str(i)\n",
    "        ######################### USE FOR en_core_web_trf model only!\n",
    "#         if len(i) > max_length:\n",
    "#             max_length = len(i)\n",
    "#         if len(i)>512:\n",
    "#             i = i[:512]\n",
    "#             count = count +1\n",
    "        #########################\n",
    "        i = nlp(i)\n",
    "        for token in i:\n",
    "            #########################\n",
    "#             token = token.lemma_\n",
    "            #########################\n",
    "            if str(token) not in prefixes and str(token) not in prefixes_end:\n",
    "                result.append(token)\n",
    "            elif str(token) in prefixes_end:\n",
    "                result.append(nlp(full_stop[0]))\n",
    "#     print(count)\n",
    "#     print(max_length)\n",
    "    return result\n",
    "\n",
    "Train_Annotation_data[\"Result\"] = sentence_to_words(Train)\n",
    "Eval_Annotation_data[\"Result\"] = sentence_to_words(Eval)\n",
    "FULL_Annotation_data[\"Result\"] = sentence_to_words(FULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "absolute-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to be annotated (Can be used for manual annotation!)\n",
    "Train_Annotation_data['Result'].to_csv('EMSCAD/Output data/Train_Annotation_data.csv')\n",
    "Eval_Annotation_data['Result'].to_csv('EMSCAD/Output data/Eval_Annotation_data.csv')\n",
    "FULL_Annotation_data['Result'].to_csv('EMSCAD/Output data/Eval_Annotation_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "tracked-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the source file that contains all biased word lists\n",
    "biased_words = pd.read_csv('EMSCAD/Input data/biased_words.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "pursuant-dispute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12649 words have been annotated.\n",
      "2867 words have been annotated.\n",
      "15516 words have been annotated.\n",
      "CPU times: user 7min 51s, sys: 1.64 s, total: 7min 53s\n",
      "Wall time: 7min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Automated annotation process (Based on the word lists imported.)\n",
    "# Only exact matches will be annotated.\n",
    "def automated_annotation(Annotation_data):\n",
    "    result = []\n",
    "    row = -1\n",
    "    count = 0\n",
    "    for i in Annotation_data['Result']:\n",
    "        i = str(i)\n",
    "        row = row + 1\n",
    "        for j in biased_words:\n",
    "            for k in biased_words[j]:\n",
    "                word = str(k)\n",
    "                if word == i:\n",
    "                    Annotation_data['Label'][row] = j\n",
    "                    count = count + 1\n",
    "    Annotation_data['Label'] = Annotation_data['Label'].fillna(\"O\")\n",
    "    print(str(count) + \" words have been annotated.\")\n",
    "    return Annotation_data\n",
    "\n",
    "Train_Annotation_data = automated_annotation(Train_Annotation_data)\n",
    "Eval_Annotation_data = automated_annotation(Eval_Annotation_data)\n",
    "FULL_Annotation_data = automated_annotation(FULL_Annotation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "monthly-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export annotated data (By \"automated\" annotator)\n",
    "Train_Annotation_data.to_csv('EMSCAD/Output data/Train_Annotation_data_output.tsv', sep='\\t', index = False, header = False)\n",
    "Eval_Annotation_data.to_csv('EMSCAD/Output data/Eval_Annotation_data_output.tsv', sep='\\t', index = False, header = False)\n",
    "FULL_Annotation_data.to_csv('EMSCAD/Output data/FULL_Annotation_data_output.tsv', sep='\\t', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "endless-simpson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O                        2419381\n",
       "Feminine-coded words        7411\n",
       "Masculine-coded words       5238\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_Annotation_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "internal-alliance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O                        568705\n",
       "Feminine-coded words       1671\n",
       "Masculine-coded words      1196\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eval_Annotation_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "democratic-bachelor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O                        2988086\n",
       "Feminine-coded words        9082\n",
       "Masculine-coded words       6434\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FULL_Annotation_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-interview",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-bookmark",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
