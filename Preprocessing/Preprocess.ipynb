{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "Firstnames = pd.read_csv('EMSCAD/anonymized/Names/first_names.all.csv', delimiter=',', header = None)\n",
    "Lastnames = pd.read_csv('EMSCAD/anonymized/Names/last_names.all.csv', delimiter=',', header = None)\n",
    "Firstnames = Firstnames.drop([1], axis=1)\n",
    "Lastnames = Lastnames.drop([1], axis=1)\n",
    "Firstnames.to_csv('EMSCAD/anonymized/Names/first_names.out.csv', index = False, header = False)\n",
    "Lastnames.to_csv('EMSCAD/anonymized/Names/last_names.out.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in Firstnames[0]:\n",
    "    keyword_processor.add_keyword(str(i), \"NAME_MASKED\")\n",
    "\n",
    "for i in Lastnames[0]:\n",
    "    keyword_processor.add_keyword(str(i), \"NAME_MASKED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = keyword_processor.replace_keywords('I love Big Ivan Richard Maggy Apple and new delhi.')\n",
    "new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-looking",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = []\n",
    "for i in jobdescriptions['title']:\n",
    "    new_sentence = keyword_processor.replace_keywords(i)\n",
    "    result.append(new_sentence)\n",
    "\n",
    "jobdescriptions['title'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "legendary-fundamentals",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Datasets\n"
     ]
    }
   ],
   "source": [
    "# Loading required packages\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from names_dataset import NameDataset\n",
    "from names_dataset import NameDatasetV1\n",
    "from flashtext import KeywordProcessor\n",
    "\n",
    "# Select Spacy model\n",
    "# Efficiency\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Accuracy\n",
    "# nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "import stanza\n",
    "# stanza.download('en')       # This downloads the English models for the neural pipeline\n",
    "# nlp = stanza.Pipeline('en', processors=\"tokenize,ner\") # This sets up a default neural pipeline in English\n",
    "\n",
    "# Change working directory\n",
    "%cd '/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-tokyo",
   "metadata": {},
   "source": [
    "## 2. Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-reaction",
   "metadata": {},
   "source": [
    "### Load EMSCAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fluid-plasma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'location', 'department', 'salary_range', 'company_profile',\n",
       "       'description', 'requirements', 'benefits', 'telecommuting',\n",
       "       'has_company_logo', 'has_questions', 'employment_type',\n",
       "       'required_experience', 'required_education', 'industry', 'function',\n",
       "       'fraudulent', 'in_balanced_dataset'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the csv file into the environment\n",
    "jobdescriptions = pd.read_csv('EMSCAD/Input data/JobDescriptions_cleaned.csv', delimiter=',')\n",
    "# jobdescriptions = jobdescriptions.drop(3703)\n",
    "# jobdescriptions = jobdescriptions.drop(17513)\n",
    "# jobdescriptions = jobdescriptions.drop(17780)\n",
    "jobdescriptions.columns\n",
    "# len(jobdescriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-identification",
   "metadata": {},
   "source": [
    "### Anonymize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-respect",
   "metadata": {},
   "source": [
    "### Subset data, remain only the column description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intensive-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create name lists from source files\n",
    "\n",
    "# Firstnames = pd.read_csv('EMSCAD/anonymized/Names/first_names.all.csv', delimiter=',', header = None)\n",
    "# Lastnames = pd.read_csv('EMSCAD/anonymized/Names/last_names.all.csv', delimiter=',', header = None)\n",
    "# Firstnames = Firstnames.drop([1], axis=1)\n",
    "# Lastnames = Lastnames.drop([1], axis=1)\n",
    "# Firstnames.to_csv('EMSCAD/anonymized/Names/first_names.out.csv', index = False, header = False)\n",
    "# Lastnames.to_csv('EMSCAD/anonymized/Names/last_names.out.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "secure-language",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.98 s, sys: 194 ms, total: 4.18 s\n",
      "Wall time: 4.25 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# keyword_processor = KeywordProcessor(case_sensitive=True)\n",
    "# # Add name lists to environment\n",
    "# for i in Firstnames[0]:\n",
    "#     keyword_processor.add_keyword(str(i), \"NAME_MASKED\")\n",
    "\n",
    "# for i in Lastnames[0]:\n",
    "#     keyword_processor.add_keyword(str(i), \"NAME_MASKED\")\n",
    "    \n",
    "# jobdescriptions_clean = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "permanent-booth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.5 ms, sys: 1.24 ms, total: 26.8 ms\n",
      "Wall time: 26.2 ms\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# result = []\n",
    "# try:\n",
    "#     counter = 0\n",
    "#     for i in jobdescriptions['in_balanced_dataset']:\n",
    "#         counter = counter +1\n",
    "#         i = str(i)\n",
    "#         new_sentence = keyword_processor.replace_keywords(i)\n",
    "#         result.append(new_sentence)\n",
    "# except:\n",
    "#     print(counter)\n",
    "#     counter = counter +1\n",
    "    \n",
    "\n",
    "# jobdescriptions_clean['in_balanced_dataset'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "pregnant-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the anonymized dataset\n",
    "\n",
    "# jobdescriptions_clean.to_csv('EMSCAD/Input data/JobDescriptions_cleaned.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "large-times",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Food52, a fast-growing, -winning online food c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Organised - Focused - NAME_MASKED - Awesome!Do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our client, located in Houston, is actively se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE COMPANY: ESRI – Environmental Systems Rese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JOB TITLE: Itemization Review Manager \\nLOCATI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17875</th>\n",
       "      <td>Just in case this is the first time you’ve vis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17876</th>\n",
       "      <td>\\nThe Payroll Accountant will focus primarily...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17877</th>\n",
       "      <td>Experienced Project Cost Control Staff Enginee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17878</th>\n",
       "      <td>NAME_MASKED Studios is looking for an experien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17879</th>\n",
       "      <td>Who are we? \\nNAME_MASKED is an award winning ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17880 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             description\n",
       "0      Food52, a fast-growing, -winning online food c...\n",
       "1      Organised - Focused - NAME_MASKED - Awesome!Do...\n",
       "2      Our client, located in Houston, is actively se...\n",
       "3      THE COMPANY: ESRI – Environmental Systems Rese...\n",
       "4      JOB TITLE: Itemization Review Manager \\nLOCATI...\n",
       "...                                                  ...\n",
       "17875  Just in case this is the first time you’ve vis...\n",
       "17876   \\nThe Payroll Accountant will focus primarily...\n",
       "17877  Experienced Project Cost Control Staff Enginee...\n",
       "17878  NAME_MASKED Studios is looking for an experien...\n",
       "17879  Who are we? \\nNAME_MASKED is an award winning ...\n",
       "\n",
       "[17880 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy column description from DF jobdescription\n",
    "descriptions = jobdescriptions['description']\n",
    "\n",
    "# Convert Series into Dataframe\n",
    "descriptions = descriptions.to_frame()\n",
    "descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-lyric",
   "metadata": {},
   "source": [
    "## 3. Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-strip",
   "metadata": {},
   "source": [
    "### Remove HTML patterns in job descriptions\n",
    "\n",
    "### Once cleaned, we can put the data through Spacy's NLP pipeline and tokenize each description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-valley",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "metropolitan-christopher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.16 s, sys: 20.1 ms, total: 5.18 s\n",
      "Wall time: 5.19 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Food52, a fast-growing, -winning online food c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Organised - Focused - NAME_MASKED - Awesome!Do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our client, located in Houston, is actively se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE COMPANY: ESRI – Environmental Systems Rese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JOB TITLE: Itemization Review Manager  LOCATIO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description\n",
       "0  Food52, a fast-growing, -winning online food c...\n",
       "1  Organised - Focused - NAME_MASKED - Awesome!Do...\n",
       "2  Our client, located in Houston, is actively se...\n",
       "3  THE COMPANY: ESRI – Environmental Systems Rese...\n",
       "4  JOB TITLE: Itemization Review Manager  LOCATIO..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Remove HTML codes based on pattern cleanr\n",
    "# Remove all characters except whitespace an alphabetic characters.\n",
    "\n",
    "# SpaCy\n",
    "# def remove_names(i):\n",
    "#     counter = 0\n",
    "#     for ent in i.ents:\n",
    "#         if ent.label_ == \"PERSON\":\n",
    "#             print(ent.label_ + \" - \" + ent.text)\n",
    "#             ent = str(ent.text)\n",
    "#             i = str(i)\n",
    "# #             print(i)\n",
    "#             j = re.sub(ent, '', i)\n",
    "#             i = j\n",
    "#             print(i)\n",
    "#     return str(i)\n",
    "\n",
    "# Stanza\n",
    "# def remove_names(i, j):\n",
    "#     k = j\n",
    "#     counter = 0\n",
    "#     for ent in i.ents:\n",
    "#         if ent.type == \"PERSON\":\n",
    "#             print(ent.type + \" - \" + ent.text)\n",
    "#             ent = str(ent.text)\n",
    "#             i = str(i)\n",
    "#             l = k.replace(ent, '')\n",
    "#             k = l\n",
    "#         else:\n",
    "#             ent = str(ent.text)\n",
    "#             threshold = 40\n",
    "#             first_name_pred = m.search_first_name(ent)\n",
    "#             last_name_pred = m.search_last_name(ent)\n",
    "#             if first_name_pred > threshold:\n",
    "#                 l = k.replace(ent, 'NAME_MASKED')\n",
    "#                 k = l\n",
    "#                 print(ent + \" - \" + str(first_name_pred))\n",
    "#             elif last_name_pred > threshold:\n",
    "#                 l = k.replace(ent, 'NAME_MASKED')\n",
    "#                 k = l\n",
    "#                 print(ent + \" - \" + str(last_name_pred))\n",
    "#     return str(k)\n",
    "\n",
    "\n",
    "result = []\n",
    "counter = 0\n",
    "# ['title', 'location', 'department', 'salary_range', 'company_profile',\n",
    "#        'description', 'requirements', 'benefits', 'telecommuting',\n",
    "#        'has_company_logo', 'has_questions', 'employment_type',\n",
    "#        'required_experience', 'required_education', 'industry', 'function',\n",
    "#        'fraudulent', 'in_balanced_dataset'],\n",
    "#       dtype='object')\n",
    "\n",
    "Cleaned = pd.DataFrame()\n",
    "for i in descriptions['description']:\n",
    "    i = str(i)\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    i = re.sub(cleanr, '', i)\n",
    "    cleanr = re.compile('<[^>]+>')\n",
    "    i = re.sub(cleanr, '', i)\n",
    "    # Remove all between hashtag (# #)\n",
    "    i = re.sub(r'#[\\w-]+#', '#URL_MASKED#', i)\n",
    "    i = re.sub(r'#[\\w\\s-]+#', '#URL_MASKED#', i)\n",
    "    # URL universal\n",
    "    i = re.sub(r\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'.,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\", '#URL_MASKED#', i)\n",
    "    # URL pattern\n",
    "    i = re.sub(r'(#\\s*URL_).[^#]+\\s*[#]', '#URL_MASKED#', i)\n",
    "    # Email patten\n",
    "    i = re.sub(r'(#\\s*EMAIL_).[^#]+\\s*[#]', '#EMAIL_MASKED#', i)\n",
    "    # Email adresses\n",
    "    i = re.sub(r'/^([a-z0-9_\\.-]+\\@[\\da-z-]+\\.[a-z\\.]{2,6})$/', '#EMAIL_MASKED#', i)\n",
    "    # Phone pattern\n",
    "    i = re.sub(r'^[+]*[(]{0,1}[0-9]{1,4}[)]{0,1}[-\\s\\./0-9]*$', '#PHONE_MASKED#', i)\n",
    "    # All phone numbers\n",
    "    i = re.sub(r'[+]*[(]{0,1}[0-9]{1,4}[)]{0,1}[-\\s\\./0-9]*$', '#PHONE_MASKED#', i)\n",
    "    i = i.replace('\\xa0', ' ')\n",
    "    i = i.replace('\\r', ' ')\n",
    "    i = i.replace('\\n', ' ')\n",
    "    i = i.replace('&amp', ' ')\n",
    "    i = i.replace('\\N{SOFT HYPHEN}', '')\n",
    "    j = str(i)\n",
    "    i = str(i)\n",
    "#     i = nlp(i)\n",
    "#     k = remove_names(i, j)\n",
    "    result.append(i)\n",
    "\n",
    "# Add the result\n",
    "Cleaned[\"description\"] = result\n",
    "Cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aging-experience",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 197 ms, sys: 35.4 ms, total: 232 ms\n",
      "Wall time: 237 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Write to csv\n",
    "# Cleaned['Result'].to_csv('EMSCAD/anonymized/EMSCAD_anonymized.csv')\n",
    "\n",
    "# Read from csv\n",
    "Cleaned = pd.read_csv('EMSCAD/anonymized/EMSCAD_anonymized.csv', delimiter=',', header = 0, index_col = 0)\n",
    "# display(Cleaned['Result'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-pilot",
   "metadata": {},
   "source": [
    "### Split descriptions into sentences\n",
    "### By doing so, we prepare the data for annotation and training the custom NER model ---> Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Split each description into sentences\n",
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "endpoint = ('.')\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [token for token in sentences if not token.startswith(prefixes)]\n",
    "    return sentences\n",
    "\n",
    "def remove_invalid_sentences(sentences):\n",
    "    prefixes = ('.')\n",
    "    output = []\n",
    "    sentences = [token for token in sentences if not token.startswith(prefixes)]\n",
    "    output.append(sentences)\n",
    "    return output\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in Cleaned[\"Result\"]:\n",
    "    sentences = str(i)\n",
    "    sentences = split_into_sentences(sentences)\n",
    "#     sentences = remove_invalid_sentences(sentences)\n",
    "    if sentences:\n",
    "        result.append(sentences)\n",
    "\n",
    "sentences = pd.DataFrame(columns=['sentence'])\n",
    "sentences[\"sentence\"] = result\n",
    "sentences['sentence'].to_csv('EMSCAD/Output data/sentence.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-arthritis",
   "metadata": {},
   "source": [
    "### TRAIN/ TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% / 20% split\n",
    "Train, Eval = train_test_split(sentences, test_size=0.2, shuffle=False)\n",
    "Train = list(Train['sentence'])\n",
    "Eval = list(Eval['sentence'])\n",
    "FULL = list(sentences['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-elimination",
   "metadata": {},
   "source": [
    "## 4. Preparation and annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Train_Annotation_data = pd.DataFrame(columns=['Result', 'Label'])\n",
    "Eval_Annotation_data = pd.DataFrame(columns=['Result', 'Label'])\n",
    "FULL_Annotation_data = pd.DataFrame(columns=['Result', 'Label'])\n",
    "#########################\n",
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "#########################\n",
    "\n",
    "prefixes = [\"\\\"\",\"#\",\"$\",\"%\",\"&\",\"'\",\"(\",\")\",\"*\",\"+\",\",\",\" \",\"-\",\"/\",\":\",\";\",\"<\",\"=\",\">\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\"`\",\"{\",\"|\",\"}\",\"~\"]\n",
    "prefixes_end = [\"?\",\"!\"]\n",
    "full_stop = [\".\"]\n",
    "gdpr_begin = [\"URL\", \"#URL\", \"EMAIL\", \"#EMAIL\", \"PHONE\", \"#PHONE\"]\n",
    "gdpr_begin = \"(URL|#URL|EMAIL|#EMAIL|PHONE|#PHONE)\"\n",
    "\n",
    "def sentence_to_words(input_list):\n",
    "    prefixes = [\"\\\"\",\"#\",\"$\",\"%\",\"&\",\"'\",\"(\",\")\",\"*\",\"+\",\",\",\" \",\"-\",\"/\",\":\",\";\",\"<\",\"=\",\">\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\"`\",\"{\",\"|\",\"}\",\"~\"]\n",
    "    prefixes_end = [\"?\",\"!\"]\n",
    "    full_stop = [\".\"]\n",
    "    result = []\n",
    "    max_length = 0\n",
    "    count = 0\n",
    "    for i in input_list:\n",
    "        i = str(i)\n",
    "        ######################### USE FOR en_core_web_trf model only!\n",
    "#         if len(i) > max_length:\n",
    "#             max_length = len(i)\n",
    "#         if len(i)>512:\n",
    "#             i = i[:512]\n",
    "#             count = count +1\n",
    "        #########################\n",
    "        i = nlp(i)\n",
    "        for token in i:\n",
    "            #########################\n",
    "            token = token.lemma_\n",
    "            #########################\n",
    "            if str(token) not in prefixes and str(token) not in prefixes_end:\n",
    "                result.append(token)\n",
    "            elif str(token) in prefixes_end:\n",
    "                result.append(nlp(full_stop[0]))\n",
    "#     print(count)\n",
    "#     print(max_length)\n",
    "    return result\n",
    "\n",
    "Train_Annotation_data[\"Result\"] = sentence_to_words(Train)\n",
    "Eval_Annotation_data[\"Result\"] = sentence_to_words(Eval)\n",
    "FULL_Annotation_data[\"Result\"] = sentence_to_words(FULL)\n",
    "\n",
    "# Train = sentence_to_words(Train)\n",
    "# Eval = sentence_to_words(Eval)\n",
    "# FULL = sentence_to_words(FULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove masked elements\n",
    "# Train_Annotation_data_clean = pd.DataFrame(columns=['Result', 'Label'])\n",
    "# Eval_Annotation_data_clean = pd.DataFrame(columns=['Result', 'Label'])\n",
    "# FULL_Annotation_data_clean = pd.DataFrame(columns=['Result', 'Label'])\n",
    "\n",
    "# gdpr_begin = [\"URL\", \"#URL\", \"EMAIL\", \"#EMAIL\", \"PHONE\", \"#PHONE\", \"url\", \"#url\", \"email\", \"#email\", \"phone\", \"#phone\"]\n",
    "\n",
    "\n",
    "# def remove_masked(input_column):\n",
    "#     count0 = 0\n",
    "#     count1 = 0\n",
    "#     count2 = 0\n",
    "#     result = []\n",
    "#     # Voor alle woorden in input_column\n",
    "#     for i in input_column:\n",
    "#         count0 = count0 +1\n",
    "#         # Voor elk type substring\n",
    "#         for j in gdpr_begin:\n",
    "#             # Als substring in i is, MASKED appenden\n",
    "#             if j in str(i):\n",
    "#                 result.append(\"MASKED\")\n",
    "#                 count1 = count1 +1\n",
    "#                 print(j)\n",
    "#                 print(i)\n",
    "#             else:\n",
    "#                 result.append(i)\n",
    "#                 count2 = count2 +1\n",
    "#     print(count2)\n",
    "#     return result\n",
    "\n",
    "\n",
    "# # def remove_masked(input_column):\n",
    "# #     for i in input_column:\n",
    "# #         counter = 0\n",
    "# #         if \"url\" not in str(i) == True:\n",
    "# #             print(\"HOI\")\n",
    "# #             result.append(i)\n",
    "# #     return result\n",
    "\n",
    "\n",
    "# # def remove_masked(input_column):\n",
    "# #     for i in input_column:\n",
    "# #         if not str(i) in (tuple(gdpr_begin)):\n",
    "# #             result.append(i)\n",
    "# #     return result\n",
    "\n",
    "# Train_Annotation_data_clean[\"Result\"] = remove_masked(Train_Annotation_data[\"Result\"])\n",
    "# Eval_Annotation_data_clean[\"Result\"] = remove_masked(Eval_Annotation_data[\"Result\"])\n",
    "# FULL_Annotation_data_clean[\"Result\"] = remove_masked(FULL_Annotation_data[\"Result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-inquiry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(result)\n",
    "len(FULL_Annotation_data_clean)\n",
    "# FULL_Annotation_data_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to be annotated (Can be used for manual annotation!)\n",
    "Train_Annotation_data['Result'].to_csv('EMSCAD/Output data/Train_Annotation_data.csv')\n",
    "Eval_Annotation_data['Result'].to_csv('EMSCAD/Output data/Eval_Annotation_data.csv')\n",
    "FULL_Annotation_data['Result'].to_csv('EMSCAD/Output data/Full_Annotation_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the source file that contains all biased word lists\n",
    "biased_words = pd.read_csv('EMSCAD/Input data/biased_words.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Automated annotation process (Based on the word lists imported.)\n",
    "# Only exact matches will be annotated.\n",
    "def automated_annotation(Annotation_data):\n",
    "    result = []\n",
    "    row = -1\n",
    "    count = 0\n",
    "    for i in Annotation_data['Result']:\n",
    "        i = str(i)\n",
    "        row = row + 1\n",
    "        for j in biased_words:\n",
    "            for k in biased_words[j]:\n",
    "                word = str(k)\n",
    "                if word == i:\n",
    "                    Annotation_data['Label'][row] = j\n",
    "                    count = count + 1\n",
    "    Annotation_data['Label'] = Annotation_data['Label'].fillna(\"O\")\n",
    "    print(str(count) + \" words have been annotated.\")\n",
    "    return Annotation_data\n",
    "\n",
    "Train_Annotation_data = automated_annotation(Train_Annotation_data)\n",
    "Eval_Annotation_data = automated_annotation(Eval_Annotation_data)\n",
    "FULL_Annotation_data = automated_annotation(FULL_Annotation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export annotated data (By \"automated\" annotator)\n",
    "Train_Annotation_data.to_csv('EMSCAD/Output data/Train_Annotation_data_output.tsv', sep='\\t', index = False, header = False)\n",
    "Eval_Annotation_data.to_csv('EMSCAD/Output data/Eval_Annotation_data_output.tsv', sep='\\t', index = False, header = False)\n",
    "FULL_Annotation_data.to_csv('EMSCAD/Output data/FULL_Annotation_data_output.tsv', sep='\\t', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Annotation_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eval_Annotation_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_Annotation_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-darwin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-moral",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
