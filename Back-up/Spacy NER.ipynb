{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wooden-sucking",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Import libraries and requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sustained-caution",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Datasets\n"
     ]
    }
   ],
   "source": [
    "# Loading required packages\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import csv\n",
    "import ast\n",
    "\n",
    "# Select Spacy model\n",
    "# Efficiency\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Accuracy\n",
    "# nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Change working directory\n",
    "%cd '/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-insertion",
   "metadata": {},
   "source": [
    "## 2. Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-boutique",
   "metadata": {},
   "source": [
    "### Load EMSCAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "intense-ethnic",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17880"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the csv file into the environment\n",
    "jobdescriptions = pd.read_csv('EMSCAD/Input data/JobDescriptions.csv', delimiter=',')\n",
    "jobdescriptions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-kuwait",
   "metadata": {},
   "source": [
    "### Subset data, remain only the column description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "forty-coverage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;p&gt;Food52, a fast-growing, James Beard Award-w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;p&gt;Organised - Focused - Vibrant - Awesome!&lt;br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;p&gt;Our client, located in Houston, is actively...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;p&gt;&lt;b&gt;THE COMPANY: ESRI – Environmental System...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;p&gt;&lt;b&gt;JOB TITLE:&lt;/b&gt; Itemization Review Manage...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description\n",
       "0  <p>Food52, a fast-growing, James Beard Award-w...\n",
       "1  <p>Organised - Focused - Vibrant - Awesome!<br...\n",
       "2  <p>Our client, located in Houston, is actively...\n",
       "3  <p><b>THE COMPANY: ESRI – Environmental System...\n",
       "4  <p><b>JOB TITLE:</b> Itemization Review Manage..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy column description from DF jobdescription\n",
    "descriptions = jobdescriptions['description']\n",
    "\n",
    "# Convert Series into Dataframe\n",
    "descriptions = descriptions.to_frame()\n",
    "descriptions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-reception",
   "metadata": {},
   "source": [
    "## 3. Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-thunder",
   "metadata": {},
   "source": [
    "### Remove HTML patterns in job descriptions\n",
    "\n",
    "### Once cleaned, we can put the data through Spacy's NLP pipeline and tokenize each description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "neither-literature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15136\n",
      "CPU times: user 365 ms, sys: 10 ms, total: 375 ms\n",
      "Wall time: 378 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Food52, a fast-growing, James Beard Award-winn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Organised - Focused - Vibrant - Awesome!Do you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our client, located in Houston, is actively se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE COMPANY: ESRI – Environmental Systems Rese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JOB TITLE: Itemization Review Manager \\nLOCATI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Result\n",
       "0  Food52, a fast-growing, James Beard Award-winn...\n",
       "1  Organised - Focused - Vibrant - Awesome!Do you...\n",
       "2  Our client, located in Houston, is actively se...\n",
       "3  THE COMPANY: ESRI – Environmental Systems Rese...\n",
       "4  JOB TITLE: Itemization Review Manager \\nLOCATI..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Remove HTML codes based on pattern cleanr\n",
    "# Remove all characters except whitespace an alphabetic characters.\n",
    "\n",
    "result = []\n",
    "docDF = pd.DataFrame()\n",
    "max_length = 0\n",
    "for i in descriptions[\"description\"]:\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    i = re.sub(cleanr, '', i)\n",
    "    cleanr = re.compile('<[^>]+>')\n",
    "    i = re.sub(cleanr, '', i)\n",
    "    i = i.replace('\\xa0', ' ')\n",
    "    i = i.replace('\\r', ' ')\n",
    "    i = i.replace('&amp', ' ')\n",
    "    i = i.replace('\\N{SOFT HYPHEN}', '')\n",
    "    doc = i\n",
    "    if len(doc) > max_length:\n",
    "        max_length = len(doc)\n",
    "    result.append(doc)\n",
    "\n",
    "print(max_length)\n",
    "    \n",
    "# Add the result\n",
    "docDF[\"Result\"] = result\n",
    "docDF.head(5)\n",
    "\n",
    "# %%time\n",
    "# # Remove HTML codes based on pattern cleanr\n",
    "# # Remove all characters except whitespace an alphabetic characters.\n",
    "\n",
    "# result = []\n",
    "# docDF = pd.DataFrame()\n",
    "# count = 0\n",
    "# for i in descriptions[\"description\"]:\n",
    "#     cleanr = re.compile('<.*?>')\n",
    "#     i = re.sub(cleanr, '', i)\n",
    "#     cleanr = re.compile('<[^>]+>')\n",
    "#     i = re.sub(cleanr, '', i)\n",
    "#     i = i.replace('\\xa0', ' ')\n",
    "#     i = i.replace('\\r', ' ')\n",
    "#     i = i.replace('&amp', ' ')\n",
    "#     i = i.replace('\\N{SOFT HYPHEN}', '')\n",
    "#     if len(i)>512:\n",
    "#         text = i[:512]\n",
    "#         count = count +1\n",
    "#     else:\n",
    "#         text = i\n",
    "#     doc = nlp(str(text))\n",
    "#     result.append(doc)\n",
    "# print(count)\n",
    "\n",
    "# # Add the result\n",
    "# docDF[\"Result\"] = result\n",
    "# docDF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-planner",
   "metadata": {},
   "source": [
    "### Split descriptions into sentences\n",
    "### By doing so, we prepare the data for annotation and training the custom NER model ---> Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "finnish-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "# # for sent in doc.sents:\n",
    "# #     print(sent.text)\n",
    "\n",
    "# for i in docDF[\"Result\"]:\n",
    "#     for sent in i.sents:\n",
    "# #         print(sent.text)\n",
    "#         result.append(sent.text)\n",
    "    \n",
    "# sentences = pd.DataFrame(columns=['sentence'])\n",
    "# sentences[\"sentence\"] = result\n",
    "\n",
    "# type(sentences[\"sentence\"])\n",
    "\n",
    "# sentences[\"sentence\"].head(10)\n",
    "\n",
    "# sentences['sentence'].to_csv('EMSCAD/Output data/sentence.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "everyday-minnesota",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.51 s, sys: 145 ms, total: 3.65 s\n",
      "Wall time: 3.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Split each description into sentences\n",
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in docDF[\"Result\"]:\n",
    "    i = str(i)\n",
    "    sentences = split_into_sentences(i)\n",
    "    if len(sentences)>0:\n",
    "        result.append(sentences)\n",
    "\n",
    "sentences = pd.DataFrame(columns=['sentence'])\n",
    "sentences[\"sentence\"] = result\n",
    "sentences['sentence'].to_csv('EMSCAD/Output data/sentence.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-rolling",
   "metadata": {},
   "source": [
    "# SKIP CLEANING PROCESS DOWN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "minimal-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to skip the cleaning process\n",
    "# sentences = pd.read_csv('EMSCAD/Output data/sentence.csv', delimiter=',', names = ['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "going-invite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Food52, a fast-growing, James Beard Award-win...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Organised - Focused - Vibrant - Awesome!, Do ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Our client, located in Houston, is actively s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[THE COMPANY: ESRI – Environmental Systems Res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[JOB TITLE: Itemization Review Manager  LOCATI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Job Overview  Apex is an environmental consul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Who is Airenvy?, Hey there!, We are seasoned ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[The Customer Service Associate will be based ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Position : #URL_86fd830a95a64e2b30ceed829e63f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[TransferWise is the clever new way to move mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence\n",
       "0  [Food52, a fast-growing, James Beard Award-win...\n",
       "1  [Organised - Focused - Vibrant - Awesome!, Do ...\n",
       "2  [Our client, located in Houston, is actively s...\n",
       "3  [THE COMPANY: ESRI – Environmental Systems Res...\n",
       "4  [JOB TITLE: Itemization Review Manager  LOCATI...\n",
       "5  [Job Overview  Apex is an environmental consul...\n",
       "6  [Who is Airenvy?, Hey there!, We are seasoned ...\n",
       "7  [The Customer Service Associate will be based ...\n",
       "8  [Position : #URL_86fd830a95a64e2b30ceed829e63f...\n",
       "9  [TransferWise is the clever new way to move mo..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80% / 20% split\n",
    "Train, Eval = train_test_split(sentences, test_size=0.2, shuffle=False)\n",
    "Train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-azerbaijan",
   "metadata": {},
   "source": [
    "### Now that we created sentences, we can also see that not everything went right\n",
    "### We remove each \"sentence\" that begins with a prefix: \".\"\n",
    "### This action takes care of removing all invalid sentences from the dataset, for each description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "significant-exploration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53 ms, sys: 2.45 ms, total: 55.4 ms\n",
      "Wall time: 54.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "Train_list = []\n",
    "Eval_list = []\n",
    "def remove_invalid_sentences(sentences):\n",
    "    prefixes = ('.')\n",
    "    tokens = []\n",
    "    output = []\n",
    "    for i in sentences[\"sentence\"]:\n",
    "        tokens = [token for token in i if not token.startswith(prefixes)]\n",
    "        output.append(tokens)\n",
    "    return output\n",
    "\n",
    "Train_list = remove_invalid_sentences(Train)\n",
    "\n",
    "Eval_list = remove_invalid_sentences(Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "regulation-brief",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(docDF)\n",
    "\n",
    "# Cleaned = pd.DataFrame()\n",
    "# type(Cleaned)\n",
    "\n",
    "# Pos = pd.DataFrame()\n",
    "# type(Pos)\n",
    "# # cols = ['DocObject']\n",
    "# # tempdocDF = pd.DataFrame(columns=cols)\n",
    "# # type(tempdocDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bearing-nerve",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Niet meer nodig omdat speciale tekens behouden blijven vanwege de zinnen.\n",
    "\n",
    "# # %%time\n",
    "\n",
    "# # Use Spacy to select the tokens using criteria\n",
    "# result = []\n",
    "# for i in docDF[\"Result\"]:\n",
    "#     i = [token.orth_ for token in i if not token.is_punct | token.is_space | token.is_stop | token.is_bracket | token.like_url | token.like_email | token.is_digit | token.is_currency] \n",
    "#     result.append(i)\n",
    "# Cleaned[\"Result\"] = result   \n",
    "# print(Cleaned)\n",
    "\n",
    "\n",
    "# # %%time\n",
    "\n",
    "# # # Use Spacy to select the tokens using criteria\n",
    "# # result = []\n",
    "# # for i in docDF[\"Result\"]:\n",
    "# #     i = [token.orth_ for token in i if not token.is_punct | token.is_space | token.is_stop | token.is_bracket | token.like_url | token.like_email | token.is_digit | token.is_currency] \n",
    "# #     result.append(i)\n",
    "# # Cleaned[\"Result\"] = result   \n",
    "# # print(Cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-millennium",
   "metadata": {},
   "source": [
    "## 4. Prepare TRAIN_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-implementation",
   "metadata": {},
   "source": [
    "### We do this by splitting each sentence on tokens and putting them all in one column, creating a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "motivated-suspension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10037\n",
      "15128\n",
      "2614\n",
      "5806\n",
      "CPU times: user 4min 1s, sys: 16.3 s, total: 4min 17s\n",
      "Wall time: 4min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Train_Annotation_data = pd.DataFrame(pd.DataFrame(columns=['Result', 'Label']))\n",
    "Eval_Annotation_data = pd.DataFrame(pd.DataFrame(columns=['Result', 'Label']))\n",
    "#########################\n",
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "#########################\n",
    "\n",
    "prefixes = [\"\\\"\",\"#\",\"$\",\"%\",\"&\",\"'\",\"(\",\")\",\"*\",\"+\",\",\",\" \",\"-\",\"/\",\":\",\";\",\"<\",\"=\",\">\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\"`\",\"{\",\"|\",\"}\",\"~\"]\n",
    "prefixes_end = [\"?\",\"!\"]\n",
    "full_stop = [\".\"]\n",
    "\n",
    "def sentence_to_words(input_list):\n",
    "    prefixes = [\"\\\"\",\"#\",\"$\",\"%\",\"&\",\"'\",\"(\",\")\",\"*\",\"+\",\",\",\" \",\"-\",\"/\",\":\",\";\",\"<\",\"=\",\">\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\"`\",\"{\",\"|\",\"}\",\"~\"]\n",
    "    prefixes_end = [\"?\",\"!\"]\n",
    "    full_stop = [\".\"]\n",
    "    result = []\n",
    "    max_length = 0\n",
    "    count = 0\n",
    "    for i in input_list:\n",
    "        i = str(i)\n",
    "        ######################### USE FOR en_core_web_trf model only!\n",
    "        if len(i) > max_length:\n",
    "            max_length = len(i)\n",
    "        if len(i)>512:\n",
    "            i = i[:512]\n",
    "            count = count +1\n",
    "        #########################\n",
    "        i = nlp(i)\n",
    "        for token in i:\n",
    "            #########################\n",
    "            token = token.lemma_\n",
    "            #########################\n",
    "            if str(token) not in prefixes and str(token) not in prefixes_end:\n",
    "                result.append(token)\n",
    "            elif str(token) in prefixes_end:\n",
    "                result.append(nlp(full_stop[0]))\n",
    "    print(count)\n",
    "    print(max_length)\n",
    "    return result\n",
    "\n",
    "Train_Annotation_data[\"Result\"] = sentence_to_words(Train_list)\n",
    "Eval_Annotation_data[\"Result\"] = sentence_to_words(Eval_list)\n",
    "\n",
    "# %%time\n",
    "# result = []\n",
    "# Annotation_data = pd.DataFrame(pd.DataFrame(columns=['Result', 'Label']))\n",
    "# #########################\n",
    "# # lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "\n",
    "\n",
    "# prefixes = [\"\\\"\",\"#\",\"$\",\"%\",\"&\",\"'\",\"(\",\")\",\"*\",\"+\",\",\",\" \",\"-\",\"/\",\":\",\";\",\"<\",\"=\",\">\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\"`\",\"{\",\"|\",\"}\",\"~\"]\n",
    "# prefixes_end = [\"?\",\"!\"]\n",
    "# full_stop = [\".\"]\n",
    "# # test = raw_input(test)\n",
    "# # full_stop[0] = full_stop[0].replace('\"', '')\n",
    "# for i in tokens_total:\n",
    "#     i = str(i)\n",
    "#     i = nlp(i)\n",
    "# #     print(i)\n",
    "#     for token in i:\n",
    "#         #########################\n",
    "# #         token = token.lemma_\n",
    "#         if str(token) not in prefixes and str(token) not in prefixes_end:\n",
    "#             result.append(token)\n",
    "#         elif str(token) in prefixes_end:\n",
    "#             result.append(nlp(full_stop[0]))\n",
    "# #         print(token)\n",
    "            \n",
    "# Annotation_data[\"Result\"] = result\n",
    "# Annotation_data[\"Result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "hungry-stress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to be annotated (Can be used for manual annotation!)\n",
    "Train_Annotation_data['Result'].to_csv('EMSCAD/Output data/Train_Annotation_data.csv')\n",
    "Eval_Annotation_data['Result'].to_csv('EMSCAD/Output data/Eval_Annotation_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "found-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the source file that contains all biased word lists\n",
    "biased_words = pd.read_csv('EMSCAD/Input data/biased_words.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "foster-ceramic",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mautomated_annotation\u001b[0;34m(Annotation_data)\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcacher_needs_updating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_with_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_maybe_update_cacher\u001b[0;34m(self, clear, verify_is_copy)\u001b[0m\n\u001b[1;32m   3471\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3472\u001b[0m                     \u001b[0;31m# otherwise, either self or ref has swapped in new arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3473\u001b[0;31m                     \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cache_changed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcacher\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3474\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m                     \u001b[0;31m# GH#33675 we have swapped in a new array, so parent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_maybe_cache_changed\u001b[0;34m(self, item, value)\u001b[0m\n\u001b[1;32m   3429\u001b[0m         \"\"\"\n\u001b[1;32m   3430\u001b[0m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3431\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36miset\u001b[0;34m(self, loc, value)\u001b[0m\n\u001b[1;32m   1108\u001b[0m             \u001b[0mblk_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblklocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m                 \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                 \u001b[0munfit_mgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblk_locs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mset_inplace\u001b[0;34m(self, locs, values)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mcreate\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0malways\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \"\"\"\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Automated annotation process (Based on the word lists imported.)\n",
    "# Only exact matches will be annotated.\n",
    "def automated_annotation(Annotation_data):\n",
    "    result = []\n",
    "    row = -1\n",
    "    count = 0\n",
    "    for i in Annotation_data['Result']:\n",
    "        i = str(i)\n",
    "        row = row + 1\n",
    "        for j in biased_words:\n",
    "            for k in biased_words[j]:\n",
    "                word = str(k)\n",
    "                if word == i:\n",
    "                    Annotation_data['Label'][row] = j\n",
    "                    count = count + 1\n",
    "    Annotation_data['Label'] = Annotation_data['Label'].fillna(\"O\")\n",
    "    print(str(count) + \" words have been annotated.\")\n",
    "    return Annotation_data\n",
    "\n",
    "Train_Annotation_data = automated_annotation(Train_Annotation_data)\n",
    "Eval_Annotation_data = automated_annotation(Eval_Annotation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "considerable-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export annotated data (By \"automated\" annotator)\n",
    "Train_Annotation_data.to_csv('EMSCAD/Output data/Train_Annotation_data_output.tsv', sep='\\t', index = False, header = False)\n",
    "Eval_Annotation_data.to_csv('EMSCAD/Output data/Eval_Annotation_data_output.tsv', sep='\\t', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-spotlight",
   "metadata": {},
   "source": [
    "### Convert tsv to JSON format --> Used to be the input for Spacy V2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "separated-partition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.22 s, sys: 97.3 ms, total: 3.32 s\n",
      "Wall time: 3.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "def tsv_to_json_format(input_path,output_path,unknown_label):\n",
    "    try:\n",
    "        f=open(input_path,'r') # input file\n",
    "        fp=open(output_path, 'w') # output file\n",
    "        data_dict={}\n",
    "        annotations =[]\n",
    "        label_dict={}\n",
    "        s=''\n",
    "        start=0\n",
    "        for line in f:\n",
    "            if line[0:len(line)-1]!='.\\tO':\n",
    "                word,entity=line.split('\\t')\n",
    "                s+=word+\" \"\n",
    "                entity=entity[:len(entity)-1]\n",
    "                if entity!=unknown_label:\n",
    "                    if len(entity) != 1:\n",
    "                        d={}\n",
    "                        d['text']=word\n",
    "                        d['start']=start\n",
    "                        d['end']=start+len(word)-1\n",
    "                        try:\n",
    "                            label_dict[entity].append(d)\n",
    "                        except:\n",
    "                            label_dict[entity]=[]\n",
    "                            label_dict[entity].append(d)\n",
    "                start+=len(word)+1\n",
    "            else:\n",
    "                data_dict['content']=s\n",
    "                s=''\n",
    "                label_list=[]\n",
    "                for ents in list(label_dict.keys()):\n",
    "                    for i in range(len(label_dict[ents])):\n",
    "                        if(label_dict[ents][i]['text']!=''):\n",
    "                            l=[ents,label_dict[ents][i]]\n",
    "                            for j in range(i+1,len(label_dict[ents])): \n",
    "                                if(label_dict[ents][i]['text']==label_dict[ents][j]['text']):  \n",
    "                                    di={}\n",
    "                                    di['start']=label_dict[ents][j]['start']\n",
    "                                    di['end']=label_dict[ents][j]['end']\n",
    "                                    di['text']=label_dict[ents][i]['text']\n",
    "                                    l.append(di)\n",
    "                                    label_dict[ents][j]['text']=''\n",
    "                            label_list.append(l)                         \n",
    "                            \n",
    "                for entities in label_list:\n",
    "                    label={}\n",
    "                    label['label']=[entities[0]]\n",
    "                    label['points']=entities[1:]\n",
    "                    annotations.append(label)\n",
    "                data_dict['annotation']=annotations\n",
    "                annotations=[]\n",
    "                json.dump(data_dict, fp)\n",
    "                fp.write('\\n')\n",
    "                data_dict={}\n",
    "                start=0\n",
    "                label_dict={}\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process file\" + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "\n",
    "tsv_to_json_format('EMSCAD/Output data/Train_Annotation_data_output.tsv','EMSCAD/Input model/Train_Annotation_data_model_input.json','abc')\n",
    "tsv_to_json_format('EMSCAD/Output data/Eval_Annotation_data_output.tsv','EMSCAD/Input model/Eval_Annotation_data_model_input.json','abc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-begin",
   "metadata": {},
   "source": [
    "# Convert JSON to SpaCy V2 format\n",
    "## We need to do this by OS Terminal!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-obligation",
   "metadata": {},
   "source": [
    "# Convert SpaCy V2 format --> SpaCy V3 format\n",
    "### First we need to import the data the data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "given-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import file\n",
    "Train_Spacy_file = open(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Datasets/EMSCAD/Input model/Train_Spacy_v2_format.txt\", \"r\")\n",
    "Eval_Spacy_file = open(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Datasets/EMSCAD/Input model/Eval_Spacy_v2_format.txt\", \"r\")\n",
    "TRAIN_DATA = ast.literal_eval(Train_Spacy_file.read())\n",
    "EVAL_DATA = ast.literal_eval(Eval_Spacy_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We transform the imported file back to tuple format.\n",
    "\n",
    "# TRAIN_DATA = ast.literal_eval(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-shakespeare",
   "metadata": {},
   "source": [
    "### Actual conversion of the data to SpaCy V3 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "important-penny",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42947/42947 [00:43<00:00, 983.93it/s] \n",
      "100%|██████████| 11573/11573 [00:12<00:00, 963.52it/s] \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "def SpaCy_v3_format(DATA,FILENAME):\n",
    "    nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "    db = DocBin() # create a DocBin object\n",
    "\n",
    "    for text, annot in tqdm(DATA): # data in previous format\n",
    "        doc = nlp.make_doc(text) # create doc object from text\n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print(\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents # label the text with the ents\n",
    "        db.add(doc)\n",
    "\n",
    "    db.to_disk(FILENAME) # save the docbin object\n",
    "    \n",
    "\n",
    "SpaCy_v3_format(TRAIN_DATA,\"EMSCAD/TRAIN_EVAL_DATA/train.spacy\")\n",
    "SpaCy_v3_format(EVAL_DATA,\"EMSCAD/TRAIN_EVAL_DATA/eval.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-nepal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "surgical-uncertainty",
   "metadata": {},
   "source": [
    "# Model evaluation is done down here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-median",
   "metadata": {},
   "source": [
    "### There is a evaluation cell for each model. We make use of evaluation data that is not seen by the model during training. The job description used for evaluation contains biased words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-passport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation data\n",
    "Eval_file = open(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Model evaluation/Evaluation dataset.txt\", \"r\")\n",
    "EVAL_DATA = Eval_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-apple",
   "metadata": {},
   "source": [
    "### Evaluation - Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-bleeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_1 = spacy.load(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Model training/Model 1/output/model-best/\") #load the best model\n",
    "doc = Model_1(EVAL_DATA) # input sample text\n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-arkansas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "collected-dependence",
   "metadata": {},
   "source": [
    "### Evaluation - Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_2 = spacy.load(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Model training/Model 2/output/model-best/\") #load the best model\n",
    "doc = Model_2(EVAL_DATA) # input sample text\n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-couple",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "laden-reputation",
   "metadata": {},
   "source": [
    "### Evaluation - Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_3 = spacy.load(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Model training/Model 3/output/model-best/\") #load the best model\n",
    "doc = Model_3(EVAL_DATA) # input sample text\n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-album",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ethical-spain",
   "metadata": {},
   "source": [
    "### Evaluation - Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_4 = spacy.load(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Model training/Model 4/output/model-best/\") #load the best model\n",
    "doc = Model_4(EVAL_DATA) # input sample text\n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-italy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "charitable-retention",
   "metadata": {},
   "source": [
    "### Evaluation - Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_5 = spacy.load(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Model training/Model 5/output/model-best/\") #load the best model\n",
    "doc = Model_5(EVAL_DATA) # input sample text\n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-variety",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-belize",
   "metadata": {},
   "source": [
    "### Evaluation - Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-passage",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_6 = spacy.load(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Model training/Model 6/output/model-best/\") #load the best model\n",
    "doc = Model_6(EVAL_DATA) # input sample text\n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-presentation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "french-swing",
   "metadata": {},
   "source": [
    "### Evaluation - Model 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-canadian",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_7 = spacy.load(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Model training/Model 7/output/model-best/\") #load the best model\n",
    "doc = Model_7(EVAL_DATA) # input sample text\n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-butterfly",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "running-audio",
   "metadata": {},
   "source": [
    "### Evaluation - Model 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_8 = spacy.load(\"/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Model training/Model 8/output/model-best/\") #load the best model\n",
    "doc = Model_8(EVAL_DATA) # input sample text\n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-cliff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-intermediate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-jacksonville",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-stephen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-lincoln",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-panama",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-gates",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-business",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-benefit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-rebecca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-adelaide",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-benefit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TRAIN_DATA[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-harvest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(\"ner\", before=\"lemmatizer\")\n",
    "\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spacy(data, iterations):\n",
    "    TRAIN_DATA = data\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        print(ner)\n",
    "        nlp.add_pipe(\"ner\", last=True)\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(iterations):\n",
    "            print(\"Starting iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                nlp.update(\n",
    "                [text],\n",
    "                [annotations],\n",
    "                drop=0.2,\n",
    "                sgd=optimizer,\n",
    "                losses=losses\n",
    "                )\n",
    "                print(losses)\n",
    "    return(nlp)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-wayne",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = TRAIN_DATA_1\n",
    "\n",
    "nlp = train_spacy(TRAIN_DATA, 30)\n",
    "nlp.to_disk(\"ner_model_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-modification",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-reservoir",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-encoding",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-april",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-accountability",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-warehouse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-concept",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-ownership",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-spectrum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-walter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-supervisor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"en_core_web_sm\"\n",
    "\n",
    "# Setting up the pipeline and entity recognizer.\n",
    "if model is not None:\n",
    "    nlp = spacy.load(model)  # load existing spacy model\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    print(\"Created blank 'en' model\")\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner)\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = 'Masculine-coded words'\n",
    "LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-march",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add new entity labels to entity recognizer\n",
    "for i in LABEL:\n",
    "    ner.add_label(i)\n",
    "# Inititalizing optimizer\n",
    "if model is None:\n",
    "    optimizer = nlp.begin_training()\n",
    "else:\n",
    "    optimizer = nlp.entity.create_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import pickle\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "# New entity labels\n",
    "# Specify the new entity labels which you want to add here\n",
    "LABEL = ['I-geo', 'B-geo', 'I-art', 'B-art', 'B-tim', 'B-nat', 'B-eve', 'O', 'I-per', 'I-tim', 'I-nat', 'I-eve', 'B-per', 'I-org', 'B-gpe', 'B-org', 'I-gpe']\n",
    "\n",
    "\"\"\"\n",
    "geo = Geographical Entity\n",
    "org = Organization\n",
    "per = Person\n",
    "gpe = Geopolitical Entity\n",
    "tim = Time indicator\n",
    "art = Artifact\n",
    "eve = Event\n",
    "nat = Natural Phenomenon\n",
    "\"\"\"\n",
    "# Loading training data \n",
    "with open ('Data/ner_corpus_260', 'rb') as fp:\n",
    "    TRAIN_DATA = pickle.load(fp)\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    new_model_name=(\"New model name for model meta.\", \"option\", \"nm\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int))\n",
    "\n",
    "def main(model=None, new_model_name='new_model', output_dir=None, n_iter=10):\n",
    "    \"\"\"Setting up the pipeline and entity recognizer, and training the new entity.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spacy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    for i in LABEL:\n",
    "        ner.add_label(i)   # Add new entity labels to entity recognizer\n",
    "\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # Get names of other pipes to disable them during training to train only NER\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "            print('Losses', losses)\n",
    "\n",
    "    # Test the trained model\n",
    "    test_text = 'Gianni Infantino is the president of FIFA.'\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # Save model \n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # Test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        doc2 = nlp2(test_text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Jcharis/Natural-Language-Processing-Tutorials/blob/master/Training%20the%20Named%20Entity%20Recognizer%20in%20SpaCy.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new entity label\n",
    "LABEL = 'Masculine-coded words'\n",
    "LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-fraud",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = Spacy_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    new_model_name=(\"New model name for model meta.\", \"option\", \"nm\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int))\n",
    "\n",
    "\n",
    "def main(model=None, new_model_name='Masculine-coded words', output_dir=None, n_iter=20):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    ner.add_label(LABEL)   # add new entity label to entity recognizer\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        # Note that 'begin_training' initializes the models, so it'll zero out\n",
    "        # existing entity types.\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in tqdm(TRAIN_DATA):\n",
    "                nlp.update([text], [annotations], sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "            print(losses)\n",
    "\n",
    "    # test the trained model\n",
    "    test_text = 'Do you like horses?'\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        doc2 = nlp2(test_text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     plac.call(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run our Function\n",
    "main(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training additional entity types using spaCy\n",
    "from __future__ import unicode_literals, print_function\n",
    "import pickle\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "# New entity labels\n",
    "# Specify the new entity labels which you want to add here\n",
    "LABEL = ['Masculine-coded words', 'Feminine-coded words']\n",
    "\n",
    "\"\"\"\n",
    "geo = Geographical Entity\n",
    "org = Organization\n",
    "per = Person\n",
    "gpe = Geopolitical Entity\n",
    "tim = Time indicator\n",
    "art = Artifact\n",
    "eve = Event\n",
    "nat = Natural Phenomenon\n",
    "\"\"\"\n",
    "# Loading training data \n",
    "with open ('/Users/richardfrissen/Documents/Maastricht University/Thesis/Development/Datasets/EMSCAD/Input model/Annotation_data_model_input_spacy.txt', 'rb') as fp:\n",
    "    TRAIN_DATA = pickle.load(fp)\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    new_model_name=(\"New model name for model meta.\", \"option\", \"nm\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int))\n",
    "\n",
    "def main(model=None, new_model_name='new_model', output_dir=None, n_iter=10):\n",
    "    \"\"\"Setting up the pipeline and entity recognizer, and training the new entity.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spacy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    for i in LABEL:\n",
    "        ner.add_label(i)   # Add new entity labels to entity recognizer\n",
    "\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # Get names of other pipes to disable them during training to train only NER\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "            print('Losses', losses)\n",
    "\n",
    "    # Test the trained model\n",
    "    test_text = 'Gianni Infantino is the president of FIFA.'\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # Save model \n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # Test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        doc2 = nlp2(test_text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    plac.call(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
